import os
import time
import random
from datetime import datetime, timedelta

class ScraperManager:
    """
    Manages scraping operations with automatic fallback and error recovery.
    
    Tiers:
    1. Apify Cloud (Fast, Reliable)
    2. Local Instagrapi (Backup)
    3. Selenium (Last Resort - Placeholder)
    
    Circuit Breaker:
    - Tracks consecutive Tier 1 failures.
    - If >= 3 failures, blocks Tier 1 for 10 minutes.
    """
    
    # Class-level state for Circuit Breaker
    _apify_failures = 0
    _circuit_open_until = None
    
    def __init__(self, apify_token=None):
        self.apify_token = apify_token or os.environ.get("APIFY_TOKEN")
        self.apify_client = None
        self.local_scraper = None
        
    def _is_circuit_open(self):
        """Check if Apify circuit is open (blocked)."""
        if ScraperManager._circuit_open_until:
            if datetime.now() < ScraperManager._circuit_open_until:
                return True
            else:
                # Reset after cooldown
                print("[ScraperManager] ğŸŸ¢ Circuit Breaker reset. Retrying Apify.")
                ScraperManager._circuit_open_until = None
                ScraperManager._apify_failures = 0
        return False

    def _record_failure(self, tier="apify"):
        if tier == "apify":
            ScraperManager._apify_failures += 1
            print(f"[ScraperManager] âš ï¸ Apify Failure Count: {ScraperManager._apify_failures}/3")
            if ScraperManager._apify_failures >= 3:
                cooldown_min = 10
                ScraperManager._circuit_open_until = datetime.now() + timedelta(minutes=cooldown_min)
                print(f"[ScraperManager] ğŸ”´ Circuit Breaker ACTIVATED. Apify blocked for {cooldown_min} minutes.")

    def _record_success(self, tier="apify"):
        if tier == "apify":
            ScraperManager._apify_failures = 0

    def _get_apify_scraper(self):
        if self._is_circuit_open():
            return None
            
        if not self.apify_client and self.apify_token:
            try:
                from scraper_apify import ApifyScraper
                self.apify_client = ApifyScraper(token=self.apify_token)
            except ImportError:
                print("[ScraperManager] Failed to import ApifyScraper")
        return self.apify_client

    def _get_local_scraper(self):
        if not self.local_scraper:
            try:
                from scraper import InstagramScraper
                self.local_scraper = InstagramScraper()
            except ImportError:
                print("[ScraperManager] Failed to import InstagramScraper")
        return self.local_scraper

    def fetch_posts(self, username, limit=3, output_dir=None, progress_callback=None):
        """
        Attempts to fetch posts using available scrapers in order of priority.
        """
        def report(msg, log_msg=None, type='info'):
            if progress_callback:
                progress_callback(msg, log_msg or msg, type)
            print(f"[ScraperManager] {msg}")

        # Tier 1: Apify
        apify = self._get_apify_scraper()
        if apify:
            try:
                report(f"Tier 1: Apify ìŠ¤í¬ë˜í•‘ ì‹œë„ ({username})...", "Apify Cloud ì—°ê²° ì‹œë„", "api")
                # Using list() to consume generator and ensure completion before returning
                # Note: lambda wrapper might be needed if signatures mismatch
                posts = list(apify.get_recent_posts_iter(username, limit, output_dir, lambda m, l: report(m, l, 'info')))
                
                if posts:
                    report(f"âœ… Apify ìˆ˜ì§‘ ì„±ê³µ: {len(posts)}ê°œ", "Apify ìˆ˜ì§‘ ì™„ë£Œ", "success")
                    self._record_success("apify")
                    return posts
                else:
                    report("âš ï¸ Apify ìˆ˜ì§‘ ê²°ê³¼ ì—†ìŒ, Tier 2 ì „í™˜...", "Apify ê²°ê³¼ 0ê°œ -> Fallback", "warning")
                    # Empty result might not be a failure (just no posts), but we fallback just in case
            except Exception as e:
                self._record_failure("apify")
                report(f"âŒ Apify ì‹¤íŒ¨: {e}", f"Apify ì—ëŸ¬: {e} -> Tier 2 ì „í™˜", "error")
        elif self._is_circuit_open():
            report("â›” Apify Circuit Open (Skipping Tier 1)", "Apify ì¼ì‹œ ì°¨ë‹¨ ì¤‘ (Circuit Breaker)", "warning")
        
        # Tier 2: Local Instagrapi
        report(f"Tier 2: ë¡œì»¬ ìŠ¤í¬ë˜í¼ ì „í™˜ ({username})...", "ë¡œì»¬ Instagrapi ì¤€ë¹„ ì¤‘", "warning")
        
        # Random cool-down before local fallback
        delay = random.uniform(5, 10)
        report(f"â³ ì•ˆì „ì„ ìœ„í•´ {delay:.1f}ì´ˆ ëŒ€ê¸°...", f"Cool-down: {delay:.1f}s", "info")
        time.sleep(delay)
        
        local = self._get_local_scraper()
        if local:
            try:
                # scraper.py expects progress_callback(msg, log_msg)
                def local_cb(msg, log_msg):
                    report(msg, log_msg, 'info')
                    
                posts = list(local.get_recent_posts_iter(username, limit, output_dir, local_cb))
                if posts:
                     report(f"âœ… ë¡œì»¬ ìˆ˜ì§‘ ì„±ê³µ: {len(posts)}ê°œ", "ë¡œì»¬ ìˆ˜ì§‘ ì™„ë£Œ", "success")
                     return posts
            except Exception as e:
                report(f"âŒ ë¡œì»¬ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}", f"ë¡œì»¬ ì—ëŸ¬: {e}", "error")
                
                # Tier 3: Selenium (Placeholder for now)
                # If we had it, we would call it here.
                # report("Tier 3: Selenium fallback...", "Selenium ì‹œë„", "warning")
                
                raise e # Re-raise if all available tiers fail
        else:
            raise Exception("No scraping backends available")
